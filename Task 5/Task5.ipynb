{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d258c9a-fca1-42d6-b621-2d023287607d",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "# End-to-End ML Pipeline with Scikit-learn Pipeline API\n",
    "\n",
    "# Objective\n",
    "The objective of this task was to build a reusable and production-ready machine learning pipeline for predicting customer churn. This included automating preprocessing, training multiple models, tuning hyperparameters, and exporting the final optimized pipeline for future use.\n",
    "\n",
    "# Introduction\n",
    "The task involved using the Telco Customer Churn dataset, which contains customer demographics, account details, and service usage information, along with the target label indicating whether a customer churned. The goal was to design a complete machine learning pipeline that could handle data preprocessing, model training, evaluation, and deployment in a streamlined and reusable way using Scikit-learnâ€™s Pipeline API.\n",
    "\n",
    "# Overview\n",
    "The process began with loading and cleaning the dataset, including converting the TotalCharges column to numeric values, dropping unnecessary fields like customerID, and handling missing values. The features were split into numerical and categorical variables, and separate preprocessing pipelines were applied: numerical features were imputed and scaled, while categorical features were imputed and one-hot encoded. Two classifiers were tested: Logistic Regression as a baseline linear model and Random Forest as a more powerful ensemble model. Each classifier was embedded into a pipeline that included both preprocessing and the model. Hyperparameter tuning was performed using GridSearchCV with cross-validation, optimizing for the F1 score. The best-performing model was then selected, evaluated, and saved using joblib for reusability.\n",
    "\n",
    "# Evaluation\n",
    "Logistic Regression achieved a best F1 score of 0.6069 with tuned parameters, while Random Forest achieved a higher F1 score of 0.6204. The Random Forest model also provided an accuracy of 76.69% and ROC AUC of 0.8320. The classification report indicated strong performance for non-churned customers (precision 0.88, recall 0.79, F1 0.83) and reasonable performance for churned customers (precision 0.55, recall 0.72, F1 0.62). Based on these results, the Random Forest model was selected as the best pipeline.\n",
    "\n",
    "# Summary\n",
    "This task successfully demonstrated the construction of an end-to-end machine learning pipeline using Scikit-learn. The workflow included preprocessing, training, hyperparameter tuning, and model export, ensuring the pipeline is reusable and production-ready. The Random Forest pipeline achieved the best performance and was saved for future deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336e9a6b-fb72-4474-964d-d331c636833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training logistic_regression\n",
      "logistic_regression - Best F1 Score: 0.6069\n",
      "Best parameters: {'classifier__C': 1, 'classifier__solver': 'liblinear'}\n",
      "\n",
      "Training random_forest\n",
      "random_forest - Best F1 Score: 0.6204\n",
      "Best parameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 100}\n",
      "\n",
      "Best model selected: random_forest\n",
      "Accuracy: 0.7669\n",
      "F1 Score: 0.6204\n",
      "ROC AUC: 0.8320\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      1033\n",
      "           1       0.55      0.72      0.62       374\n",
      "\n",
      "    accuracy                           0.77      1407\n",
      "   macro avg       0.72      0.75      0.73      1407\n",
      "weighted avg       0.79      0.77      0.78      1407\n",
      "\n",
      "\n",
      "Pipeline saved as telco_churn_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "     Loads the telco customer churn dataset and performs initial cleaning\n",
    "     Converts totalcharges to numeric to handle errors by invalid values\n",
    "     Removes the customer id column (not useful for prediction).\n",
    "     Drops rows with missing values.\n",
    "     Converts churn column from Yes/No to 1/0.\n",
    "    \"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    df = df.drop('customerID', axis=1)\n",
    "    df = df.dropna()\n",
    "    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 2: Create preprocessing pipeline\n",
    "def create_preprocessing_pipeline(X):\n",
    "    \"\"\"\n",
    "    Creates a preprocessing pipeline for numerical and categorical features\n",
    "    \n",
    "    \"\"\"\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Pipeline for numerical features\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Pipeline for categorical features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine both transformers into a single ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# Step 3: Main execution\n",
    "def main():\n",
    "    # Load and clean data\n",
    "    df = load_and_preprocess_data()\n",
    "    X = df.drop('Churn', axis=1)\n",
    "    y = df['Churn']\n",
    "\n",
    "    # Split into training and test sets (stratified to balance churn labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = create_preprocessing_pipeline(X)\n",
    "\n",
    "    # Define models and their hyperparamete by grids for tuning\n",
    "    models = {\n",
    "        'logistic_regression': {\n",
    "            'model': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "            'params': {'classifier__C': [0.1, 1, 10], 'classifier__solver': ['liblinear']}\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "            'params': {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [None, 10, 20]}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    best_model_name = \"\"\n",
    "\n",
    "    # Train and evaluate each model using gridsearchCV\n",
    "    for model_name, model_info in models.items():\n",
    "        print(f\"\\nTraining {model_name}\")\n",
    "\n",
    "        # Create full pipeline (preprocessing + model)\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model_info['model'])\n",
    "        ])\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            model_info['params'],\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"{model_name} - Best F1 Score: {f1:.4f}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "        # Keep track of best performing model\n",
    "        if f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_model_name = model_name\n",
    "\n",
    "    # Final evaluation of best model\n",
    "    print(f\"\\nBest model selected: {best_model_name}\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save final pipeline to disk for reuse\n",
    "    joblib.dump(best_model, 'telco_churn_pipeline.joblib')\n",
    "    print(\"\\nPipeline saved as telco_churn_pipeline.joblib\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25dc4e-bc9d-4b28-a24c-e134097bf609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
