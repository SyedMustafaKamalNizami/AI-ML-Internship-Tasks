{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4:\n",
        "# News Topic Classifier\n",
        "# Objective\n",
        "\n",
        "The objective of this task was to build a text classification system that categorizes news headlines into one of four categories (World, Sports, Business, Sci/Tech). This was achieved by fine-tuning a pretrained BERT model (bert-base-uncased) on the AG News dataset, allowing the model to learn context-specific patterns in news language for accurate classification.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This task involved applying transfer learning with the BERT model, a widely used transformer-based architecture for natural language processing. The AG News dataset, containing 120,000 training and 7,600 testing samples across four balanced categories, was used. The main goal was to demonstrate how a large pretrained model can be fine-tuned for a specific task without training from scratch. The process included tokenizing the text using the BERT tokenizer, preparing the dataset for training, fine-tuning the classification head, and finally deploying the model for real-time interaction using Gradio.\n",
        "\n",
        "# Overview\n",
        "\n",
        "The workflow began with loading the AG News dataset from Hugging Face Datasets and splitting the training set into training and validation subsets. The bert-base-uncased tokenizer converted each news headline into token IDs and attention masks suitable for BERT input. The model was fine-tuned using Hugging Faceâ€™s Trainer API.\n",
        "\n",
        "Evaluation was performed using accuracy and F1-score (weighted) to measure classification performance across all categories. The fine-tuned model and tokenizer were saved locally, and a Hugging Face pipeline was created for easy inference.\n",
        "\n",
        "For deployment, a Gradio interface was implemented, allowing users to input custom headlines and instantly view the predicted category with scores.\n",
        "\n",
        "# Summary\n",
        "\n",
        "This task successfully delivered a fine-tuned BERT-based text classifier capable of categorizing news headlines into predefined topics with strong accuracy and F1 performance. It demonstrated key skills in natural language processing, including tokenization, transformer fine-tuning, and evaluation. Furthermore, lightweight deployment through Gradio showcased how machine learning models can be made interactive and user-friendly."
      ],
      "metadata": {
        "id": "gPZGOUZunwMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages with fallbacks\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers==4.21.0 datasets accelerate gradio scikit-learn\n",
        "\n",
        "# Try to install evaluate, if it fails we'll use sklearn\n",
        "try:\n",
        "    !pip install -q evaluate\n",
        "    evaluate_available = True\n",
        "except:\n",
        "    evaluate_available = False\n",
        "    print(\"evaluate package not available, using sklearn fallback\")\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "import gradio as gr\n",
        "\n",
        "# Disable Weights & Biases logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the AG News dataset\n",
        "raw_ds = load_dataset(\"ag_news\")\n",
        "\n",
        "# Split training set into train (90%) and validation (10%)\n",
        "splits = raw_ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = splits[\"train\"]\n",
        "val_ds = splits[\"test\"]\n",
        "test_ds = raw_ds[\"test\"]\n",
        "\n",
        "# Load tokenizer for BERT\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to tokenize text data\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "# Apply tokenization to datasets\n",
        "train_tok = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "val_tok = val_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "test_tok = test_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Define label mapping\n",
        "id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# Load pretrained BERT model for classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=4,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Setup metrics with fallback\n",
        "try:\n",
        "    import evaluate\n",
        "    acc_metric = evaluate.load(\"accuracy\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        return {\n",
        "            \"accuracy\": acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "            \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
        "        }\n",
        "    print(\"Using evaluate package for metrics\")\n",
        "\n",
        "except ImportError:\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "        }\n",
        "    print(\"Using sklearn for metrics\")\n",
        "\n",
        "# Data collator for padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"bert-agnews\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    seed=42,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Create Hugging Face Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training\")\n",
        "trainer.train()\n",
        "print(\"Training complete\")\n",
        "\n",
        "# Evaluate model on validation set\n",
        "val_metrics = trainer.evaluate()\n",
        "print(\"\\nValidation Results:\")\n",
        "print(f\"Accuracy: {val_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-Score: {val_metrics['eval_f1']:.4f}\")\n",
        "\n",
        "# Evaluate model on test set\n",
        "test_metrics = trainer.evaluate(test_tok)\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"Accuracy: {test_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-Score: {test_metrics['eval_f1']:.4f}\")\n",
        "\n",
        "# Save trained model and tokenizer\n",
        "model.save_pretrained(\"./bert-news-classifier\")\n",
        "tokenizer.save_pretrained(\"./bert-news-classifier\")\n",
        "print(\"Model saved\")\n",
        "\n",
        "# Load trained model into a text classification pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"./bert-news-classifier\",\n",
        "    tokenizer=\"./bert-news-classifier\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# Function to classify a news headline\n",
        "def classify_news(text):\n",
        "    if not text.strip():\n",
        "        return \"Enter a news headline\"\n",
        "    result = classifier(text)\n",
        "    confidence = result[0]['score']\n",
        "    category = result[0]['label']\n",
        "    return f\"Category: {category}\\nConfidence: {confidence:.2%}\"\n",
        "\n",
        "# Create Gradio app for interactive testing\n",
        "demo = gr.Interface(\n",
        "    fn=classify_news,\n",
        "    inputs=gr.Textbox(label=\"News Headline\", placeholder=\"Enter news headline\"),\n",
        "    outputs=gr.Textbox(label=\"Classification Result\"),\n",
        "    title=\"News Topic Classifier\",\n",
        "    examples=[\n",
        "        \"Microsoft announces new Windows update with security fixes\",\n",
        "        \"Pakistan wins T20 cricket match against India\",\n",
        "        \"SpaceX successfully lands reusable rocket\",\n",
        "        \"Pakistan faces Flash Floods in August\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch demo with public sharing\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "XCEX-XxGoXrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}